{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Part_1_Classifiers_hw3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xuwl_kObjmqH",
        "outputId": "a194a1af-bdb5-423b-c272-ee16dd50f546"
      },
      "source": [
        "import csv\n",
        "import pandas as pd\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, BaggingClassifier\n",
        "\n",
        "\n",
        "def evaluate_model(dt_classifier, x, y, l, d):\n",
        "    y_pred=dt_classifier.predict(x)\n",
        "    print(\"Test Accuracy for Dataset:\", d, \"and\", l, \"is\", accuracy_score(y, y_pred))\n",
        "    print(\"Test Confusion Matrix:\")\n",
        "    print(confusion_matrix(y, y_pred))\n",
        "    print(\"F1 Score for Dataset:\", d, \"and\", l, \"is\", f1_score(y, y_pred, average=None))\n",
        "\n",
        "\n",
        "for i in ['_c300', '_c500', '_c1000', '_c1500', '_c1800']:\n",
        "  for j in ['_d100', '_d1000', '_d5000']:\n",
        "    for l in ['tree', 'bagging', 'randomForest', 'gradientBoosting']:\n",
        "      grid_search=None\n",
        "      params=None\n",
        "      dt_best=None\n",
        "\n",
        "      if l=='tree':\n",
        "        params = {\n",
        "        'max_depth': [2, 3, 5, 10, 20, 40, 80, 200],\n",
        "        'min_samples_leaf': [5, 10, 20, 50, 100],\n",
        "        'criterion': [\"gini\", \"entropy\"],\n",
        "        'max_features': [2, 3, 200, 300, 400, 500],\n",
        "        'splitter': [\"best\", \"random\"]\n",
        "        }\n",
        "\n",
        "        dt = DecisionTreeClassifier(random_state=42)\n",
        "        grid_search = GridSearchCV(estimator=dt, param_grid=params, cv=4, n_jobs=-1, scoring = \"accuracy\")\n",
        "\n",
        "      if l == 'bagging':\n",
        "        params = {\n",
        "        'bootstrap': [True, False],   \n",
        "        'n_estimators': [10, 35, 80, 280],\n",
        "        'base_estimator__max_depth': [2, 3, 20],\n",
        "        'base_estimator__min_samples_leaf': [5, 20, 50],\n",
        "        'base_estimator__max_features': [2, 3, 50, 300],\n",
        "        }\n",
        "        grid_search = GridSearchCV(BaggingClassifier(base_estimator=dt, random_state=42), param_grid=params, cv=3, scoring = \"accuracy\")\n",
        "\n",
        "      if l=='randomForest':\n",
        "        params= {\n",
        "        'bootstrap': [True],\n",
        "        'max_depth': [5, 10, 30, 90, 110],\n",
        "        'max_features': [2, 3, 200, 300],\n",
        "        'n_estimators': [100, 200]\n",
        "        }\n",
        "        rf = RandomForestClassifier(random_state=42)\n",
        "        grid_search = GridSearchCV(estimator=rf, param_grid=params, cv=5, scoring = \"accuracy\")\n",
        "\n",
        "      if l=='gradientBoosting':\n",
        "        gbc = GradientBoostingClassifier()\n",
        "        params = {\n",
        "          \"n_estimators\":[5,20, 50, 250],\n",
        "          \"max_depth\":[1,3,5,7,9],\n",
        "          \"learning_rate\":[0.01,0.1,1,10,20]\n",
        "        }\n",
        "        grid_search = GridSearchCV(estimator=gbc, param_grid=params, cv=5, scoring = \"accuracy\")\n",
        "\n",
        "      #add path to the datasets here\n",
        "      read_train = pd.read_csv(r'/content/drive/MyDrive/hw3_part1_data/all_data/'+'train'+i+j+'.csv', header=None)\n",
        "      read_valid = pd.read_csv(r'/content/drive/MyDrive/hw3_part1_data/all_data/'+'valid'+i+j+'.csv', header=None)\n",
        "      read_test = pd.read_csv(r'/content/drive/MyDrive/hw3_part1_data/all_data/'+'test'+i+j+'.csv', header=None)\n",
        "\n",
        "      read_valid = shuffle(read_valid)\n",
        "      X_valid=read_valid.drop(500, axis=1)\n",
        "      Y_valid=read_valid[500]\n",
        "      grid_search.fit(X_valid, Y_valid)\n",
        "\n",
        "      dt_best = grid_search.best_estimator_\n",
        "      print(\"Best Estimator\", dt_best)\n",
        "\n",
        "      X_train=read_train.drop(500, axis=1)\n",
        "      Y_train=read_train[500]\n",
        "      X = pd.concat([X_train, X_valid])\n",
        "      Y = pd.concat([Y_train, Y_valid])\n",
        "      dt_best.fit(X, Y)\n",
        "        \n",
        "      X_test=read_test.drop(500, axis=1)\n",
        "      Y_test=read_test[500]\n",
        "      evaluate_model(dt_best, X_test, Y_test, l, 'test'+i+j)\n",
        "      print(\"\\n\")\n",
        "    print(\"\\n\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Estimator RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
            "                       criterion='gini', max_depth=5, max_features=2,\n",
            "                       max_leaf_nodes=None, max_samples=None,\n",
            "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "                       min_samples_leaf=1, min_samples_split=2,\n",
            "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
            "                       n_jobs=None, oob_score=False, random_state=42, verbose=0,\n",
            "                       warm_start=False)\n",
            "Test Accuracy for Dataset: test_c1800_d5000 and randomForest is 1.0\n",
            "Test Confusion Matrix:\n",
            "[[5000    0]\n",
            " [   0 5000]]\n",
            "F1 Score for Dataset: test_c1800_d5000 and randomForest is [1. 1.]\n",
            "\n",
            "\n",
            "Best Estimator GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,\n",
            "                           learning_rate=0.1, loss='deviance', max_depth=1,\n",
            "                           max_features=None, max_leaf_nodes=None,\n",
            "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "                           min_samples_leaf=1, min_samples_split=2,\n",
            "                           min_weight_fraction_leaf=0.0, n_estimators=250,\n",
            "                           n_iter_no_change=None, presort='deprecated',\n",
            "                           random_state=None, subsample=1.0, tol=0.0001,\n",
            "                           validation_fraction=0.1, verbose=0,\n",
            "                           warm_start=False)\n",
            "Test Accuracy for Dataset: test_c1800_d5000 and gradientBoosting is 1.0\n",
            "Test Confusion Matrix:\n",
            "[[5000    0]\n",
            " [   0 5000]]\n",
            "F1 Score for Dataset: test_c1800_d5000 and gradientBoosting is [1. 1.]\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQvh62DlHUz_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVhqMV6wRNty"
      },
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "param_grid = {\n",
        " 'bootstrap': [True, False],\n",
        " 'bootstrap_features': [True, False],    \n",
        " 'n_estimators': [5, 10, 15, 20],\n",
        " 'max_samples' : [0.6, 0.8, 1.0],\n",
        " 'base_estimator__bootstrap': [True, False],    \n",
        " 'base_estimator__n_estimators': [100, 200, 300],\n",
        " 'base_estimator__max_features' : [0.6, 0.8, 1.0]\n",
        "}\n",
        "\n",
        "grid_search_bagging=GridSearchCV(BaggingClassifier(base_estimator=dt), param_grid=param_grid, cv=5, scoring = \"accuracy\")\n",
        "\n",
        "for i in ['300', '500', '1000', '1500', '1800']:\n",
        "  for j in ['_d100', '_d1000', '_d5000']:\n",
        "    for k in ['valid_c', 'train_c', 'test_c']:\n",
        "      if k=='valid_c':\n",
        "        print(i, j, k)\n",
        "        read =  pd.read_csv(r'/content/drive/MyDrive/hw3_part1_data/all_data/'+k+i+j+'.csv', header=None)\n",
        "        read = shuffle(read)\n",
        "        X_valid=read.drop(500, axis=1)\n",
        "        Y_valid=read[500]\n",
        "\n",
        "        grid_search.fit(X_valid, Y_valid)\n",
        "        score_df = pd.DataFrame(grid_search.cv_results_)\n",
        "        dt_best = grid_search.best_estimator_\n",
        "        print(\"Best Estimator\", dt_best)\n",
        "\n",
        "      if k=='train_c':\n",
        "        read =  pd.read_csv(r'/content/drive/MyDrive/hw3_part1_data/all_data/'+k+i+j+'.csv', header=None)\n",
        "      \n",
        "        X_train=read.drop(500, axis=1)\n",
        "        Y_train=read[500]\n",
        "        X = pd.concat([X_train, X_valid])\n",
        "        Y = pd.concat([Y_train, Y_valid])\n",
        "        dt_best.fit(X, Y)\n",
        "      \n",
        "      if k=='test_c':\n",
        "        read =  pd.read_csv(r'/content/drive/MyDrive/hw3_part1_data/all_data/'+k+i+j+'.csv', header=None)\n",
        "      \n",
        "        X_test=read.drop(500, axis=1)\n",
        "        Y_test=read[500]\n",
        "        evaluate_model(dt_best, X_test, Y_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFwgVd7fWG0A"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}